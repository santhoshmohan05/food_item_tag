{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MultiModalDataset\n",
    "from model import FoodItemTagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/santhosh.mohan/Downloads/DSCVAssessment/assignments/food_item_tag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_func, optim, device = 'cpu'):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_size = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        image = batch['image'].to(device)\n",
    "        price = batch['price'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        text = {}\n",
    "        text['input_ids'] = batch['input_ids'].to(device)\n",
    "        text['attention_mask'] = batch['attention_mask'].to(device)\n",
    "        text['token_type_ids'] = batch['token_type_ids'].to(device)\n",
    "        optim.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            output = model(image,text,price)\n",
    "            loss = loss_func(output, label)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        epoch_loss += loss.item() * label.size(0)\n",
    "        total_size += label.size(0)\n",
    "    epoch_loss = epoch_loss / total_size\n",
    "    print(f\"Training Loss - {epoch_loss}\")\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_func, device = 'cpu'):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_size = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        image = batch['image'].to(device)\n",
    "        price = batch['price'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        text = {}\n",
    "        text['input_ids'] = batch['input_ids'].to(device)\n",
    "        text['attention_mask'] = batch['attention_mask'].to(device)\n",
    "        text['token_type_ids'] = batch['token_type_ids'].to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            output = model(image,text,price)\n",
    "            loss = loss_func(output, label)\n",
    "        epoch_loss += loss.item() * label.size(0)\n",
    "        total_size += label.size(0)\n",
    "    epoch_loss = epoch_loss / total_size\n",
    "    print(f\"Training Loss - {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image_model(device='cpu', epochs = 10):\n",
    "    model = FoodItemTagModel(512,512,48)\n",
    "    model = model.to(device)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[40,70, 90], gamma=0.15)\n",
    "    training_dataset = MultiModalDataset(f\"{base_path}/data/training_data.csv\", f\"{base_path}/imgs\", image_transforms[\"train\"])\n",
    "    validation_dataset = MultiModalDataset(f\"{base_path}/data/validation_data.csv\", f\"{base_path}/imgs\", image_transforms[\"val\"])\n",
    "\n",
    "\n",
    "    training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=4,\n",
    "                                                 shuffle=True, num_workers=0)\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=4,\n",
    "                                                 shuffle=True, num_workers=0)#,collate_fn=lambda x: x)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch - {epoch+1}\")\n",
    "        train_epoch(model, training_dataloader, loss_func, optimizer, device)\n",
    "        validate_epoch(model, validation_dataloader, loss_func, device)\n",
    "        scheduler.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = train_image_model(epochs = 2)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c714affcd2ff116daf6a3c12c58af84c70e0d953094f3f6ad96ebc4d5924f537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
